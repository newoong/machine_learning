{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cc3dd97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sewoong\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "importing Jupyter notebook from utils.ipynb\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Time-series Generative Adversarial Networks (TimeGAN) Codebase.\n",
    "Reference: Jinsung Yoon, Daniel Jarrett, Mihaela van der Schaar, \n",
    "\"Time-series Generative Adversarial Networks,\" \n",
    "Neural Information Processing Systems (NeurIPS), 2019.\n",
    "Paper link: https://papers.nips.cc/paper/8789-time-series-generative-adversarial-networks\n",
    "Last updated Date: April 24th 2020\n",
    "Code author: Jinsung Yoon (jsyoon0823@gmail.com)\n",
    "-----------------------------\n",
    "timegan.py\n",
    "Note: Use original data as training set to generater synthetic data (time-series)\n",
    "\"\"\"\n",
    "\n",
    "# Necessary Packages\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import import_ipynb\n",
    "from utils import extract_time, rnn_cell, random_generator, batch_generator\n",
    "\n",
    "\n",
    "def timegan (ori_data, parameters):\n",
    "  \"\"\"TimeGAN function.\n",
    "  \n",
    "  Use original data as training set to generater synthetic data (time-series)\n",
    "  \n",
    "  Args:\n",
    "    - ori_data: original time-series data\n",
    "    - parameters: TimeGAN network parameters\n",
    "    \n",
    "  Returns:\n",
    "    - generated_data: generated time-series data\n",
    "  \"\"\"\n",
    "  # Initialization on the Graph\n",
    "  tf.reset_default_graph()\n",
    "\n",
    "  # Basic Parameters\n",
    "  no, seq_len, dim = np.asarray(ori_data).shape #49975, 24, 12\n",
    "    \n",
    "  # Maximum sequence length and each sequence length\n",
    "  ori_time, max_seq_len = extract_time(ori_data) #24,24\n",
    "  \n",
    "  def MinMaxScaler(data):\n",
    "    \"\"\"Min-Max Normalizer.\n",
    "    \n",
    "    Args:\n",
    "      - data: raw data\n",
    "      \n",
    "    Returns:\n",
    "      - norm_data: normalized data\n",
    "      - min_val: minimum values (for renormalization)\n",
    "      - max_val: maximum values (for renormalization)\n",
    "    \"\"\"    \n",
    "    min_val = np.min(np.min(data, axis = 0), axis = 0) #가장 작은 행\n",
    "    data = data - min_val\n",
    "      \n",
    "    max_val = np.max(np.max(data, axis = 0), axis = 0)\n",
    "    norm_data = data / (max_val + 1e-27)\n",
    "      \n",
    "    return norm_data, min_val, max_val\n",
    "  \n",
    "  # Normalization\n",
    "  ori_data, min_val, max_val = MinMaxScaler(ori_data) #MinMaxScaler를 2번?\n",
    "              \n",
    "  ## Build a RNN networks          \n",
    "  \n",
    "  # Network Parameters\n",
    "  hidden_dim   = parameters['hidden_dim'] \n",
    "  num_layers   = parameters['num_layer']\n",
    "  iterations   = parameters['iterations']\n",
    "  batch_size   = parameters['batch_size']\n",
    "  module_name  = parameters['module'] \n",
    "  z_dim        = dim\n",
    "  gamma        = 1\n",
    "    \n",
    "  # Input place holders\n",
    "  X = tf.placeholder(tf.float32, [None, max_seq_len, dim], name = \"myinput_x\") #tf.Variable(tf.ones(shape=[None, self._num_states]), dtype=tf.float32)\n",
    "  Z = tf.placeholder(tf.float32, [None, max_seq_len, z_dim], name = \"myinput_z\")\n",
    "  T = tf.placeholder(tf.int32, [None], name = \"myinput_t\")\n",
    "  \n",
    "  def embedder (X, T):\n",
    "    \"\"\"Embedding network between original feature space to latent space.\n",
    "    \n",
    "    Args:\n",
    "      - X: input time-series features\n",
    "      - T: input time information\n",
    "      \n",
    "    Returns:\n",
    "      - H: embeddings\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"embedder\", reuse = tf.AUTO_REUSE):\n",
    "      e_cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell(module_name, hidden_dim) for _ in range(num_layers)])\n",
    "      e_outputs, e_last_states = tf.nn.dynamic_rnn(e_cell, X, dtype=tf.float32, sequence_length = T)\n",
    "      H = tf.layers.dense(e_outputs, hidden_dim, activation=tf.nn.sigmoid)     \n",
    "    return H\n",
    "      \n",
    "  def recovery (H, T):   \n",
    "    \"\"\"Recovery network from latent space to original space.\n",
    "    \n",
    "    Args:\n",
    "      - H: latent representation\n",
    "      - T: input time information\n",
    "      \n",
    "    Returns:\n",
    "      - X_tilde: recovered data\n",
    "    \"\"\"     \n",
    "    with tf.variable_scope(\"recovery\", reuse = tf.AUTO_REUSE):       \n",
    "      r_cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell(module_name, hidden_dim) for _ in range(num_layers)])\n",
    "      r_outputs, r_last_states = tf.nn.dynamic_rnn(r_cell, H, dtype=tf.float32, sequence_length = T)\n",
    "      X_tilde = tf.layers.dense(r_outputs, dim, activation=tf.nn.sigmoid) \n",
    "    return X_tilde\n",
    "    \n",
    "  def generator (Z, T):  \n",
    "    \"\"\"Generator function: Generate time-series data in latent space.\n",
    "    \n",
    "    Args:\n",
    "      - Z: random variables\n",
    "      - T: input time information\n",
    "      \n",
    "    Returns:\n",
    "      - E: generated embedding\n",
    "    \"\"\"        \n",
    "    with tf.variable_scope(\"generator\", reuse = tf.AUTO_REUSE):\n",
    "      e_cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell(module_name, hidden_dim) for _ in range(num_layers)])\n",
    "      e_outputs, e_last_states = tf.nn.dynamic_rnn(e_cell, Z, dtype=tf.float32, sequence_length = T)\n",
    "      E = tf.layers.dense(e_outputs, hidden_dim, activation=tf.nn.sigmoid)     \n",
    "    return E\n",
    "      \n",
    "  def supervisor (H, T): \n",
    "    \"\"\"Generate next sequence using the previous sequence.\n",
    "    \n",
    "    Args:\n",
    "      - H: latent representation\n",
    "      - T: input time information\n",
    "      \n",
    "    Returns:\n",
    "      - S: generated sequence based on the latent representations generated by the generator\n",
    "    \"\"\"          \n",
    "    with tf.variable_scope(\"supervisor\", reuse = tf.AUTO_REUSE):\n",
    "      e_cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell(module_name, hidden_dim) for _ in range(num_layers-1)])\n",
    "      e_outputs, e_last_states = tf.nn.dynamic_rnn(e_cell, H, dtype=tf.float32, sequence_length = T)\n",
    "      S = tf.layers.dense(e_outputs, hidden_dim, activation=tf.nn.sigmoid)     \n",
    "    return S\n",
    "          \n",
    "  def discriminator (H, T):\n",
    "    \"\"\"Discriminate the original and synthetic time-series data.\n",
    "    \n",
    "    Args:\n",
    "      - H: latent representation\n",
    "      - T: input time information\n",
    "      \n",
    "    Returns:\n",
    "      - Y_hat: classification results between original and synthetic time-series\n",
    "    \"\"\"        \n",
    "    with tf.variable_scope(\"discriminator\", reuse = tf.AUTO_REUSE):\n",
    "      d_cell = tf.nn.rnn_cell.MultiRNNCell([rnn_cell(module_name, hidden_dim) for _ in range(num_layers)])\n",
    "      d_outputs, d_last_states = tf.nn.dynamic_rnn(d_cell, H, dtype=tf.float32, sequence_length = T)\n",
    "      Y_hat = tf.layers.dense(d_outputs, 1, activation=None) \n",
    "    return Y_hat   \n",
    "    \n",
    "  # Embedder & Recovery\n",
    "  H = embedder(X, T)\n",
    "  X_tilde = recovery(H, T)\n",
    "    \n",
    "  # Generator\n",
    "  E_hat = generator(Z, T)\n",
    "  H_hat = supervisor(E_hat, T)\n",
    "  H_hat_supervise = supervisor(H, T)\n",
    "    \n",
    "  # Synthetic data\n",
    "  X_hat = recovery(H_hat, T)\n",
    "    \n",
    "  # Discriminator\n",
    "  Y_fake = discriminator(H_hat, T)\n",
    "  Y_real = discriminator(H, T)     \n",
    "  Y_fake_e = discriminator(E_hat, T)\n",
    "    \n",
    "  # Variables        \n",
    "  e_vars = [v for v in tf.trainable_variables() if v.name.startswith('embedder')]\n",
    "  r_vars = [v for v in tf.trainable_variables() if v.name.startswith('recovery')]\n",
    "  g_vars = [v for v in tf.trainable_variables() if v.name.startswith('generator')]\n",
    "  s_vars = [v for v in tf.trainable_variables() if v.name.startswith('supervisor')]\n",
    "  d_vars = [v for v in tf.trainable_variables() if v.name.startswith('discriminator')]\n",
    "    \n",
    "  # Discriminator loss\n",
    "  D_loss_real = tf.losses.sigmoid_cross_entropy(tf.ones_like(Y_real), Y_real)\n",
    "  D_loss_fake = tf.losses.sigmoid_cross_entropy(tf.zeros_like(Y_fake), Y_fake)\n",
    "  D_loss_fake_e = tf.losses.sigmoid_cross_entropy(tf.zeros_like(Y_fake_e), Y_fake_e)\n",
    "  D_loss = D_loss_real + D_loss_fake + gamma * D_loss_fake_e\n",
    "            \n",
    "  # Generator loss\n",
    "  # 1. Adversarial loss\n",
    "  G_loss_U = tf.losses.sigmoid_cross_entropy(tf.ones_like(Y_fake), Y_fake)\n",
    "  G_loss_U_e = tf.losses.sigmoid_cross_entropy(tf.ones_like(Y_fake_e), Y_fake_e)\n",
    "    \n",
    "  # 2. Supervised loss\n",
    "  G_loss_S = tf.losses.mean_squared_error(H[:,1:,:], H_hat_supervise[:,:-1,:])\n",
    "    \n",
    "  # 3. Two Momments\n",
    "  G_loss_V1 = tf.reduce_mean(tf.abs(tf.sqrt(tf.nn.moments(X_hat,[0])[1] + 1e-6) - tf.sqrt(tf.nn.moments(X,[0])[1] + 1e-6)))\n",
    "  G_loss_V2 = tf.reduce_mean(tf.abs((tf.nn.moments(X_hat,[0])[0]) - (tf.nn.moments(X,[0])[0])))\n",
    "    \n",
    "  G_loss_V = G_loss_V1 + G_loss_V2\n",
    "    \n",
    "  # 4. Summation\n",
    "  G_loss = G_loss_U + gamma * G_loss_U_e + 100 * tf.sqrt(G_loss_S) + 100*G_loss_V \n",
    "            \n",
    "  # Embedder network loss\n",
    "  E_loss_T0 = tf.losses.mean_squared_error(X, X_tilde)\n",
    "  E_loss0 = 10*tf.sqrt(E_loss_T0)\n",
    "  E_loss = E_loss0  + 0.1*G_loss_S\n",
    "    \n",
    "  # optimizer\n",
    "  E0_solver = tf.train.AdamOptimizer().minimize(E_loss0, var_list = e_vars + r_vars)\n",
    "  E_solver = tf.train.AdamOptimizer().minimize(E_loss, var_list = e_vars + r_vars)\n",
    "  D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list = d_vars)\n",
    "  G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list = g_vars + s_vars)      \n",
    "  GS_solver = tf.train.AdamOptimizer().minimize(G_loss_S, var_list = g_vars + s_vars)   \n",
    "        \n",
    "  ## TimeGAN training   \n",
    "  sess = tf.Session()\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "  # 1. Embedding network training\n",
    "  print('Start Embedding Network Training')\n",
    "    \n",
    "  for itt in range(iterations):\n",
    "    # Set mini-batch\n",
    "    X_mb, T_mb = batch_generator(ori_data, ori_time, batch_size)           \n",
    "    # Train embedder        \n",
    "    _, step_e_loss = sess.run([E0_solver, E_loss_T0], feed_dict={X: X_mb, T: T_mb})        \n",
    "    # Checkpoint\n",
    "    if itt % 1000 == 0:\n",
    "      print('step: '+ str(itt) + '/' + str(iterations) + ', e_loss: ' + str(np.round(np.sqrt(step_e_loss),4)) ) \n",
    "      \n",
    "  print('Finish Embedding Network Training')\n",
    "    \n",
    "  # 2. Training only with supervised loss\n",
    "  print('Start Training with Supervised Loss Only')\n",
    "    \n",
    "  for itt in range(iterations):\n",
    "    # Set mini-batch\n",
    "    X_mb, T_mb = batch_generator(ori_data, ori_time, batch_size)    \n",
    "    # Random vector generation   \n",
    "    Z_mb = random_generator(batch_size, z_dim, T_mb, max_seq_len)\n",
    "    # Train generator       \n",
    "    _, step_g_loss_s = sess.run([GS_solver, G_loss_S], feed_dict={Z: Z_mb, X: X_mb, T: T_mb})       \n",
    "    # Checkpoint\n",
    "    if itt % 1000 == 0:\n",
    "      print('step: '+ str(itt)  + '/' + str(iterations) +', s_loss: ' + str(np.round(np.sqrt(step_g_loss_s),4)) )\n",
    "      \n",
    "  print('Finish Training with Supervised Loss Only')\n",
    "    \n",
    "  # 3. Joint Training\n",
    "  print('Start Joint Training')\n",
    "  \n",
    "  for itt in range(iterations):\n",
    "    # Generator training (twice more than discriminator training)\n",
    "    for kk in range(2):\n",
    "      # Set mini-batch\n",
    "      X_mb, T_mb = batch_generator(ori_data, ori_time, batch_size)               \n",
    "      # Random vector generation\n",
    "      Z_mb = random_generator(batch_size, z_dim, T_mb, max_seq_len)\n",
    "      # Train generator\n",
    "      _, step_g_loss_u, step_g_loss_s, step_g_loss_v = sess.run([G_solver, G_loss_U, G_loss_S, G_loss_V], feed_dict={Z: Z_mb, X: X_mb, T: T_mb})\n",
    "       # Train embedder        \n",
    "      _, step_e_loss_t0 = sess.run([E_solver, E_loss_T0], feed_dict={Z: Z_mb, X: X_mb, T: T_mb})   \n",
    "           \n",
    "    # Discriminator training        \n",
    "    # Set mini-batch\n",
    "    X_mb, T_mb = batch_generator(ori_data, ori_time, batch_size)           \n",
    "    # Random vector generation\n",
    "    Z_mb = random_generator(batch_size, z_dim, T_mb, max_seq_len)\n",
    "    # Check discriminator loss before updating\n",
    "    check_d_loss = sess.run(D_loss, feed_dict={X: X_mb, T: T_mb, Z: Z_mb})\n",
    "    # Train discriminator (only when the discriminator does not work well)\n",
    "    if (check_d_loss > 0.15):        \n",
    "      _, step_d_loss = sess.run([D_solver, D_loss], feed_dict={X: X_mb, T: T_mb, Z: Z_mb})\n",
    "        \n",
    "    # Print multiple checkpoints\n",
    "    if itt % 1000 == 0:\n",
    "      print('step: '+ str(itt) + '/' + str(iterations) + \n",
    "            ', d_loss: ' + str(np.round(step_d_loss,4)) + \n",
    "            ', g_loss_u: ' + str(np.round(step_g_loss_u,4)) + \n",
    "            ', g_loss_s: ' + str(np.round(np.sqrt(step_g_loss_s),4)) + \n",
    "            ', g_loss_v: ' + str(np.round(step_g_loss_v,4)) + \n",
    "            ', e_loss_t0: ' + str(np.round(np.sqrt(step_e_loss_t0),4))  )\n",
    "  print('Finish Joint Training')\n",
    "    \n",
    "  ## Synthetic data generation\n",
    "  Z_mb = random_generator(no, z_dim, ori_time, max_seq_len)\n",
    "  generated_data_curr = sess.run(X_hat, feed_dict={Z: Z_mb, X: ori_data, T: ori_time})    \n",
    "    \n",
    "  generated_data = list()\n",
    "    \n",
    "  for i in range(no):\n",
    "    temp = generated_data_curr[i,:ori_time[i],:]\n",
    "    generated_data.append(temp)\n",
    "        \n",
    "  # Renormalization\n",
    "  generated_data = generated_data * max_val\n",
    "  generated_data = generated_data + min_val\n",
    "    \n",
    "  return generated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec3349e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
