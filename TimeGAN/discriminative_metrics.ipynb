{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd44faaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sewoong\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "importing Jupyter notebook from utils.ipynb\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Time-series Generative Adversarial Networks (TimeGAN) Codebase.\n",
    "Reference: Jinsung Yoon, Daniel Jarrett, Mihaela van der Schaar, \n",
    "\"Time-series Generative Adversarial Networks,\" \n",
    "Neural Information Processing Systems (NeurIPS), 2019.\n",
    "Paper link: https://papers.nips.cc/paper/8789-time-series-generative-adversarial-networks\n",
    "Last updated Date: April 24th 2020\n",
    "Code author: Jinsung Yoon (jsyoon0823@gmail.com)\n",
    "-----------------------------\n",
    "predictive_metrics.py\n",
    "Note: Use post-hoc RNN to classify original data and synthetic data\n",
    "Output: discriminative score (np.abs(classification accuracy - 0.5))\n",
    "\"\"\"\n",
    "\n",
    "# Necessary Packages\n",
    "import import_ipynb\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from utils import train_test_divide, extract_time, batch_generator\n",
    "\n",
    "\n",
    "def discriminative_score_metrics (ori_data, generated_data):\n",
    "  \"\"\"Use post-hoc RNN to classify original data and synthetic data\n",
    "  \n",
    "  Args:\n",
    "    - ori_data: original data\n",
    "    - generated_data: generated synthetic data\n",
    "    \n",
    "  Returns:\n",
    "    - discriminative_score: np.abs(classification accuracy - 0.5)\n",
    "  \"\"\"\n",
    "  # Initialization on the Graph\n",
    "  tf.reset_default_graph()\n",
    "\n",
    "  # Basic Parameters\n",
    "  no, seq_len, dim = np.asarray(ori_data).shape  \n",
    "    \n",
    "  # Set maximum sequence length and each sequence length\n",
    "  ori_time, ori_max_seq_len = extract_time(ori_data)\n",
    "  generated_time, generated_max_seq_len = extract_time(ori_data)\n",
    "  max_seq_len = max([ori_max_seq_len, generated_max_seq_len])  #24\n",
    "     \n",
    "  ## Builde a post-hoc RNN discriminator network \n",
    "  # Network parameters\n",
    "  hidden_dim = int(dim/2)\n",
    "  iterations = 2000\n",
    "  batch_size = 128\n",
    "    \n",
    "  # Input place holders\n",
    "  # Feature\n",
    "  X = tf.placeholder(tf.float32, [None, max_seq_len, dim], name = \"myinput_x\")\n",
    "  X_hat = tf.placeholder(tf.float32, [None, max_seq_len, dim], name = \"myinput_x_hat\")\n",
    "    \n",
    "  T = tf.placeholder(tf.int32, [None], name = \"myinput_t\")\n",
    "  T_hat = tf.placeholder(tf.int32, [None], name = \"myinput_t_hat\")\n",
    "    \n",
    "  # discriminator function\n",
    "  def discriminator (x, t):\n",
    "    \"\"\"Simple discriminator function.\n",
    "    \n",
    "    Args:\n",
    "      - x: time-series data\n",
    "      - t: time information\n",
    "      \n",
    "    Returns:\n",
    "      - y_hat_logit: logits of the discriminator output\n",
    "      - y_hat: discriminator output\n",
    "      - d_vars: discriminator variables\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"discriminator\", reuse = tf.AUTO_REUSE) as vs:\n",
    "      d_cell = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim, activation=tf.nn.tanh, name = 'd_cell')\n",
    "      d_outputs, d_last_states = tf.nn.dynamic_rnn(d_cell, x, dtype=tf.float32, sequence_length = t)\n",
    "      y_hat_logit = tf.layers.dense(d_last_states, 1, activation=None) \n",
    "      y_hat = tf.nn.sigmoid(y_hat_logit)\n",
    "      d_vars = [v for v in tf.all_variables() if v.name.startswith(vs.name)]\n",
    "    \n",
    "    return y_hat_logit, y_hat, d_vars\n",
    "    \n",
    "  y_logit_real, y_pred_real, d_vars = discriminator(X, T)\n",
    "  y_logit_fake, y_pred_fake, _ = discriminator(X_hat, T_hat)\n",
    "        \n",
    "  # Loss for the discriminator\n",
    "  d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = y_logit_real, \n",
    "                                                                       labels = tf.ones_like(y_logit_real)))\n",
    "  d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = y_logit_fake, \n",
    "                                                                       labels = tf.zeros_like(y_logit_fake)))\n",
    "  d_loss = d_loss_real + d_loss_fake\n",
    "    \n",
    "  # optimizer\n",
    "  d_solver = tf.train.AdamOptimizer().minimize(d_loss, var_list = d_vars)\n",
    "        \n",
    "  ## Train the discriminator   \n",
    "  # Start session and initialize\n",
    "  sess = tf.Session()\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "  # Train/test division for both original and generated data\n",
    "  train_x, train_x_hat, test_x, test_x_hat, train_t, train_t_hat, test_t, test_t_hat = \\\n",
    "  train_test_divide(ori_data, generated_data, ori_time, generated_time)\n",
    "    \n",
    "  # Training step\n",
    "  for itt in range(iterations):\n",
    "          \n",
    "    # Batch setting\n",
    "    X_mb, T_mb = batch_generator(train_x, train_t, batch_size)\n",
    "    X_hat_mb, T_hat_mb = batch_generator(train_x_hat, train_t_hat, batch_size)\n",
    "          \n",
    "    # Train discriminator\n",
    "    _, step_d_loss = sess.run([d_solver, d_loss], \n",
    "                              feed_dict={X: X_mb, T: T_mb, X_hat: X_hat_mb, T_hat: T_hat_mb})            \n",
    "    \n",
    "  ## Test the performance on the testing set    \n",
    "  y_pred_real_curr, y_pred_fake_curr = sess.run([y_pred_real, y_pred_fake], \n",
    "                                                feed_dict={X: test_x, T: test_t, X_hat: test_x_hat, T_hat: test_t_hat})\n",
    "    \n",
    "  y_pred_final = np.squeeze(np.concatenate((y_pred_real_curr, y_pred_fake_curr), axis = 0))\n",
    "  y_label_final = np.concatenate((np.ones([len(y_pred_real_curr),]), np.zeros([len(y_pred_fake_curr),])), axis = 0)\n",
    "    \n",
    "  # Compute the accuracy\n",
    "  acc = accuracy_score(y_label_final, (y_pred_final>0.5))\n",
    "  discriminative_score = np.abs(0.5-acc)\n",
    "    \n",
    "  return discriminative_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
