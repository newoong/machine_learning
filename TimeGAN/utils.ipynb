{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33a0a8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Time-series Generative Adversarial Networks (TimeGAN) Codebase.\n",
    "Reference: Jinsung Yoon, Daniel Jarrett, Mihaela van der Schaar, \n",
    "\"Time-series Generative Adversarial Networks,\" \n",
    "Neural Information Processing Systems (NeurIPS), 2019.\n",
    "Paper link: https://papers.nips.cc/paper/8789-time-series-generative-adversarial-networks\n",
    "Last updated Date: April 24th 2020\n",
    "Code author: Jinsung Yoon (jsyoon0823@gmail.com)\n",
    "-----------------------------\n",
    "utils.py\n",
    "(1) train_test_divide: Divide train and test data for both original and synthetic data.\n",
    "(2) extract_time: Returns Maximum sequence length and each sequence length.\n",
    "(3) rnn_cell: Basic RNN Cell.\n",
    "(4) random_generator: random vector generator\n",
    "(5) batch_generator: mini-batch generator\n",
    "\"\"\"\n",
    "\n",
    "## Necessary Packages\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "\n",
    "def train_test_divide (data_x, data_x_hat, data_t, data_t_hat, train_rate = 0.8):\n",
    "  \"\"\"Divide train and test data for both original and synthetic data.\n",
    "  \n",
    "  Args:\n",
    "    - data_x: original data\n",
    "    - data_x_hat: generated data\n",
    "    - data_t: original time\n",
    "    - data_t_hat: generated time\n",
    "    - train_rate: ratio of training data from the original data\n",
    "  \"\"\"\n",
    "  # Divide train/test index (original data)\n",
    "  no = len(data_x)\n",
    "  idx = np.random.permutation(no)\n",
    "  train_idx = idx[:int(no*train_rate)]\n",
    "  test_idx = idx[int(no*train_rate):]\n",
    "    \n",
    "  train_x = [data_x[i] for i in train_idx]\n",
    "  test_x = [data_x[i] for i in test_idx]\n",
    "  train_t = [data_t[i] for i in train_idx]\n",
    "  test_t = [data_t[i] for i in test_idx]      \n",
    "    \n",
    "  # Divide train/test index (synthetic data)\n",
    "  no = len(data_x_hat)\n",
    "  idx = np.random.permutation(no)\n",
    "  train_idx = idx[:int(no*train_rate)]\n",
    "  test_idx = idx[int(no*train_rate):]\n",
    "  \n",
    "  train_x_hat = [data_x_hat[i] for i in train_idx]\n",
    "  test_x_hat = [data_x_hat[i] for i in test_idx]\n",
    "  train_t_hat = [data_t_hat[i] for i in train_idx]\n",
    "  test_t_hat = [data_t_hat[i] for i in test_idx]\n",
    "  \n",
    "  return train_x, train_x_hat, test_x, test_x_hat, train_t, train_t_hat, test_t, test_t_hat\n",
    "\n",
    "\n",
    "def extract_time (data):\n",
    "  \"\"\"Returns Maximum sequence length and each sequence length.\n",
    "  \n",
    "  Args:\n",
    "    - data: original data\n",
    "    \n",
    "  Returns:\n",
    "    - time: extracted time information\n",
    "    - max_seq_len: maximum sequence length\n",
    "  \"\"\"\n",
    "  time = list()\n",
    "  max_seq_len = 0\n",
    "  for i in range(len(data)):\n",
    "    max_seq_len = max(max_seq_len, len(data[i][:,0]))\n",
    "    time.append(len(data[i][:,0]))\n",
    "    \n",
    "  return time, max_seq_len\n",
    "\n",
    "\n",
    "def rnn_cell(module_name, hidden_dim):\n",
    "  \"\"\"Basic RNN Cell.\n",
    "    \n",
    "  Args:\n",
    "    - module_name: gru, lstm, or lstmLN\n",
    "    \n",
    "  Returns:\n",
    "    - rnn_cell: RNN Cell\n",
    "  \"\"\"\n",
    "  assert module_name in ['gru','lstm','lstmLN']\n",
    "  \n",
    "  # GRU\n",
    "  if (module_name == 'gru'):\n",
    "    rnn_cell = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim, activation=tf.nn.tanh)\n",
    "  # LSTM\n",
    "  elif (module_name == 'lstm'):\n",
    "    rnn_cell = tf.nn.rnn_cell.LSTMCell(num_units=hidden_dim, activation=tf.nn.tanh)\n",
    "  # LSTM Layer Normalization\n",
    "  elif (module_name == 'lstmLN'):\n",
    "    rnn_cell = tf.contrib.rnn.LayerNormBasicLSTMCell(num_units=hidden_dim, activation=tf.nn.tanh)\n",
    "  return rnn_cell\n",
    "\n",
    "\n",
    "def random_generator (batch_size, z_dim, T_mb, max_seq_len):\n",
    "  \"\"\"Random vector generation.\n",
    "  \n",
    "  Args:\n",
    "    - batch_size: size of the random vector\n",
    "    - z_dim: dimension of random vector\n",
    "    - T_mb: time information for the random vector\n",
    "    - max_seq_len: maximum sequence length\n",
    "    \n",
    "  Returns:\n",
    "    - Z_mb: generated random vector\n",
    "  \"\"\"\n",
    "  Z_mb = list()\n",
    "  for i in range(batch_size):\n",
    "    temp = np.zeros([max_seq_len, z_dim])\n",
    "    temp_Z = np.random.uniform(0., 1, [T_mb[i], z_dim])\n",
    "    temp[:T_mb[i],:] = temp_Z\n",
    "    Z_mb.append(temp_Z)\n",
    "  return Z_mb\n",
    "\n",
    "\n",
    "def batch_generator(data, time, batch_size):\n",
    "  \"\"\"Mini-batch generator.\n",
    "  \n",
    "  Args:\n",
    "    - data: time-series data\n",
    "    - time: time information\n",
    "    - batch_size: the number of samples in each batch\n",
    "    \n",
    "  Returns:\n",
    "    - X_mb: time-series data in each batch\n",
    "    - T_mb: time information in each batch\n",
    "  \"\"\"\n",
    "  no = len(data)\n",
    "  idx = np.random.permutation(no)\n",
    "  train_idx = idx[:batch_size]     \n",
    "            \n",
    "  X_mb = list(data[i] for i in train_idx)\n",
    "  T_mb = list(time[i] for i in train_idx)\n",
    "  \n",
    "  return X_mb, T_mb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
