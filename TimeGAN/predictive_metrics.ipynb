{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb353a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Time-series Generative Adversarial Networks (TimeGAN) Codebase.\n",
    "Reference: Jinsung Yoon, Daniel Jarrett, Mihaela van der Schaar, \n",
    "\"Time-series Generative Adversarial Networks,\" \n",
    "Neural Information Processing Systems (NeurIPS), 2019.\n",
    "Paper link: https://papers.nips.cc/paper/8789-time-series-generative-adversarial-networks\n",
    "Last updated Date: April 24th 2020\n",
    "Code author: Jinsung Yoon (jsyoon0823@gmail.com)\n",
    "-----------------------------\n",
    "predictive_metrics.py\n",
    "Note: Use Post-hoc RNN to predict one-step ahead (last feature)\n",
    "\"\"\"\n",
    "\n",
    "# Necessary Packages\n",
    "import import_ipynb\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from utils import extract_time\n",
    "\n",
    " \n",
    "def predictive_score_metrics (ori_data, generated_data):\n",
    "  \"\"\"Report the performance of Post-hoc RNN one-step ahead prediction.\n",
    "  \n",
    "  Args:\n",
    "    - ori_data: original data\n",
    "    - generated_data: generated synthetic data\n",
    "    \n",
    "  Returns:\n",
    "    - predictive_score: MAE of the predictions on the original data\n",
    "  \"\"\"\n",
    "  # Initialization on the Graph\n",
    "  tf.compat.v1.reset_default_graph()\n",
    "\n",
    "  # Basic Parameters\n",
    "  no, seq_len, dim = np.asarray(ori_data).shape\n",
    "    \n",
    "  # Set maximum sequence length and each sequence length\n",
    "  ori_time, ori_max_seq_len = extract_time(ori_data)\n",
    "  generated_time, generated_max_seq_len = extract_time(ori_data)\n",
    "  max_seq_len = max([ori_max_seq_len, generated_max_seq_len])  #24\n",
    "     \n",
    "  ## Builde a post-hoc RNN predictive network \n",
    "  # Network parameters\n",
    "  hidden_dim = int(dim/2)\n",
    "  iterations = 5000\n",
    "  batch_size = 128\n",
    "    \n",
    "  # Input place holders\n",
    "  X = tf.placeholder(tf.float32, [None, max_seq_len-1, dim-1], name = \"myinput_x\")\n",
    "  T = tf.placeholder(tf.int32, [None], name = \"myinput_t\")    \n",
    "  Y = tf.placeholder(tf.float32, [None, max_seq_len-1, 1], name = \"myinput_y\")\n",
    "    \n",
    "  # Predictor function\n",
    "  def predictor (x, t):\n",
    "    \"\"\"Simple predictor function.\n",
    "    \n",
    "    Args:\n",
    "      - x: time-series data\n",
    "      - t: time information\n",
    "      \n",
    "    Returns:\n",
    "      - y_hat: prediction\n",
    "      - p_vars: predictor variables\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(\"predictor\", reuse = tf.AUTO_REUSE) as vs:\n",
    "      p_cell = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim, activation=tf.nn.tanh, name = 'p_cell')\n",
    "      p_outputs, p_last_states = tf.nn.dynamic_rnn(p_cell, x, dtype=tf.float32, sequence_length = t)\n",
    "      y_hat_logit = tf.layers.dense(p_outputs, 1, activation=None) \n",
    "      y_hat = tf.nn.sigmoid(y_hat_logit)\n",
    "      p_vars = [v for v in tf.all_variables() if v.name.startswith(vs.name)]\n",
    "    \n",
    "    return y_hat, p_vars\n",
    "    \n",
    "  y_pred, p_vars = predictor(X, T)\n",
    "  # Loss for the predictor\n",
    "  p_loss = tf.losses.absolute_difference(Y, y_pred)\n",
    "  # optimizer\n",
    "  p_solver = tf.train.AdamOptimizer().minimize(p_loss, var_list = p_vars)\n",
    "        \n",
    "  ## Training    \n",
    "  # Session start\n",
    "  sess = tf.Session()\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "  # Training using Synthetic dataset\n",
    "  for itt in range(iterations):\n",
    "          \n",
    "    # Set mini-batch\n",
    "    idx = np.random.permutation(len(generated_data))\n",
    "    train_idx = idx[:batch_size]     \n",
    "            \n",
    "    X_mb = list(generated_data[i][:-1,:(dim-1)] for i in train_idx)\n",
    "    T_mb = list(generated_time[i]-1 for i in train_idx)\n",
    "    Y_mb = list(np.reshape(generated_data[i][1:,(dim-1)],[len(generated_data[i][1:,(dim-1)]),1]) for i in train_idx)        \n",
    "          \n",
    "    # Train predictor\n",
    "    _, step_p_loss = sess.run([p_solver, p_loss], feed_dict={X: X_mb, T: T_mb, Y: Y_mb})        \n",
    "    \n",
    "  ## Test the trained model on the original data\n",
    "  idx = np.random.permutation(len(ori_data))\n",
    "  train_idx = idx[:no]\n",
    "    \n",
    "  X_mb = list(ori_data[i][:-1,:(dim-1)] for i in train_idx)\n",
    "  T_mb = list(ori_time[i]-1 for i in train_idx)\n",
    "  Y_mb = list(np.reshape(ori_data[i][1:,(dim-1)], [len(ori_data[i][1:,(dim-1)]),1]) for i in train_idx)\n",
    "    \n",
    "  # Prediction\n",
    "  pred_Y_curr = sess.run(y_pred, feed_dict={X: X_mb, T: T_mb})\n",
    "    \n",
    "  # Compute the performance in terms of MAE\n",
    "  MAE_temp = 0\n",
    "  for i in range(no):\n",
    "    MAE_temp = MAE_temp + mean_absolute_error(Y_mb[i], pred_Y_curr[i,:,:])\n",
    "    \n",
    "  predictive_score = MAE_temp / no\n",
    "    \n",
    "  return predictive_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
